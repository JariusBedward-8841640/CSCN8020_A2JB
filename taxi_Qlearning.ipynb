{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CSCN8020 Q Learning Taxi\n",
    "\n",
    "\n",
    "The environment we define will contain:\n",
    "- 500 discrete states\n",
    "- 6 discrete actions\n",
    "- Reward structure:\n",
    "    - -1 per step\n",
    "    - +20 for correct passenger drop-off\n",
    "    - -10 for illegal pickup or drop-off\n",
    "\n",
    "The goal is to learn the optimal policy that maximizes cumulative reward by efficiently transporting passengers.\n",
    "\n",
    "Legend:\n",
    "- Agent: Taxi\n",
    "- Enviornment: Taxi-v3 grid world\n",
    "- State Space: 500 discrete states\n",
    "- Action Space: 6 discrete actions\n",
    "- Policy: ε-greedy\n",
    "- Value function: Q(s,a)\n",
    "- Learning Type: Model free, off-policy, Temporal difference\n",
    "\n",
    "The Q-Learning update rule:\n",
    "Q(s,a) ← Q(s,a) + α [r + γ max_a' Q(s',a') − Q(s,a)]\n",
    "\n",
    "Where:\n",
    "- α = learning rate\n",
    "- γ  = discount factor\n",
    "- ε =  exploration factor ( ε-greedy policy)\n",
    "\n"
   ],
   "id": "8f8619179a913935"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Import & Setup",
   "id": "b58532571a0734c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from idlelib.rpc import request_queue\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename='taxi.log',\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format=\"%(message)s\"\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Imports explanation:\n",
    "- gymnasium: provides Taxi environment for experiment\n",
    "- numpy: used for Q-table and math operations\n",
    "- random: used for epsilon-greedy exploration\n",
    "- logging: used to store metrics into a file that can be used to analyze and compare info\n",
    "- matplotlib: used for plotting performance curves\n",
    "\n",
    "The Q-table will be a 500 x 6 matrix\n",
    "Each row corresponds to a state while each column corresponds to an action."
   ],
   "id": "bc90b68e2ba6a559"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Initialize Environment & Analysis\n",
    "\n",
    "Observation space: 500 states\n",
    "\n",
    "State encoding:\n",
    "((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination\n",
    "\n",
    "Breakdown:\n",
    "- 25 taxi positions\n",
    "- 5 passenger locations\n",
    "- 4 destination locations\n",
    "\n",
    "Action Space (6 actions):\n",
    "0: South (Down)\n",
    "1: North (Up)\n",
    "2: East (Right)\n",
    "3: West (Left)\n",
    "4: Pickup passenger\n",
    "5: Drop off passenger\n",
    "\n",
    "Reward Structure:\n",
    "- -1per step\n",
    "- +20 successful passenger drop-off\n",
    "- -10 illegal pickup or drop off\n",
    "\n",
    "The step penalty encourages shortest-path behavior\n",
    "The +20 reward propagates backward via bootstrapping which is to say, the agent updates its current state-value estimate based on the estimated value of the previous state. Over multiple episodes the +20 reward is passed backward a step at a time which eventually points directly toward the goal."
   ],
   "id": "527d81b580bcc2e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n #.n indicates that the environment is using a discrete space and the .n stands for the number and it returns the absolute count of discrete elements within that specific space\n",
    "\n",
    "print(\"State Size: \", state_size)\n",
    "print(\"Action Size: \", action_size)"
   ],
   "id": "1d24eb14fb884633"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Q-Learning Implementation\n",
    "\n",
    "We implement tabular Q-Learning:\n",
    "This means the agent uses a 2D matrix(states as rows, actions as columns) to store expected future rewards, continuously updating these values through experience to learn the best possible action for any given situation\n",
    "\n",
    "Q-table dimensions\n",
    "500 x 6\n",
    "\n",
    "Policy:\n",
    "ε-greedy\n",
    "\n",
    "Update:\n",
    "Q(s,a) += α [ r + γ max Q(s',a') − Q(s,a) ]\n",
    "\n",
    "The TD error is\n",
    "δ = r + γ max Q(s',a') − Q(s,a)\n",
    "\n",
    "If δ > 0 → underestimation\n",
    "If δ < 0 → overestimation\n",
    "\n",
    "The algorithm is model-free and off-policy.\n"
   ],
   "id": "f4d28094f93a71c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_q_learning(alpha, gamma, epsilon, episodes=5000):\n",
    "\n",
    "    Q  = np.zeros([state_size, action_size])\n",
    "\n",
    "    steps_per_episode = []\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            #ε-greedy action selection\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            #Q-Update\n",
    "            Q[state, action] += alpha * (\n",
    "                reward + gamma * np.max(Q[next_state]) - Q[state, action]\n",
    "            ) # This line of code adjusts the current q-value byt taking a small step (determined by the learning rate alpha) toward the newly observed reality, which is the immediate reward plus the best possible estimated future value\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        steps_per_episode.append(steps)\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "        logger.info(f\"{alpha},{gamma},{epsilon},{episode},{steps},{total_reward}\")\n",
    "\n",
    "    return Q, steps_per_episode, rewards_per_episode\n",
    "\n"
   ],
   "id": "42308a8aaae9acf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Metric Reporting\n",
    "\n",
    "The following metrics will be reported:\n",
    "1. Total Episodes\n",
    "2. Total steps per episode\n",
    "3. Average return per episode\n",
    "Each is computed and loged for each experiment"
   ],
   "id": "1cc9b13728c7462f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def summarize_results(name, steps, rewards):\n",
    "\n",
    "    total_episodes = len(rewards)\n",
    "    avg_return = np.mean(rewards)\n",
    "    avg_steps = np.mean(steps)\n",
    "\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(\"Total Episodes:\", total_episodes)\n",
    "    print(\"Average Return:\", avg_return)\n",
    "    print(\"Average Steps:\", avg_steps)\n",
    "\n",
    "    logger.info(f\"\\n===== {name} =====\")\n",
    "    logger.info(f\"Total Episodes: {total_episodes}\")\n",
    "    logger.info(f\"Average Return: {avg_return}\")\n",
    "    logger.info(f\"Average Steps: {avg_steps}\")"
   ],
   "id": "90fd37ceac900166"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Base Training run",
   "id": "dc30ae0c48fc1511"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "alpha = 0.1 #Learning Rate\n",
    "gamma = 0.9 # Discount Factor\n",
    "epsilon = 0.1 #Exploration Rate\n",
    "\n",
    "Q_base, steps_base, rewards_base, = train_q_learning(alpha, gamma, epsilon)\n",
    "\n",
    "summarize_results(\"Base_Run\", steps_base, rewards_base)"
   ],
   "id": "43569a26a824c63d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Plot Base Metrics",
   "id": "5027c68f2622ff20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure()\n",
    "plt.plot(rewards_base)\n",
    "plt.title(\"Base Run: Reward per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(steps_base)\n",
    "plt.title(\"Base Run: Steps per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.show()"
   ],
   "id": "151a2fca93ec73d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
